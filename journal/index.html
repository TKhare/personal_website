<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Tejas Khare</title>
  <style>
    body {
      font-family: sans-serif;
      line-height: 1.5;
    }
    .container {
      max-width: 600px;
      margin: 0 auto;
      padding: 20px;
    }
    h1, h2 {
      margin-top: 40px;
    }
    hr {
      margin: 20px 0;
    }
    a {
    }
    a:hover {
      text-decoration: underline;
    }
    ul {
      list-style-type: none;
      padding: 0;
      margin: 0;
    }
    li {
      margin-bottom: 8px;
    }
  </style>
</head>
<body>
  <div class="container">
    <a href="/" style="text-decoration: none; color: inherit">← Home</a>
 <h1>Journal</h1>
    <h2>Daily notes</h2>
    <hr>
        <b>08-31-2025 ‣ </b>Worked on quantization-aware training (QAT) and LoRA on GPT-2, exploring training stability and performance across bit-widths. Cyclic precision training notably improved lower precision settings (likely by encouraging wider minima), with a 5-bit model outperforming the 8-bit baseline. <a href="https://github.com/TKhare/QAT_LoRA_gpt2">Repo</a>
    <br>
        <b>08-15-2025 ‣ </b>Spent time revisiting <a href="https://docs.google.com/document/d/1HBdClydi9yagHKQcUlLzjf9EMJDC2lgI9AGhp9uP7A0/edit?usp=sharing">transformer math</a>; clarified how feed-forward layers are independent per-token while attention introduces cross-token dependencies, and how multi-head attention improves expressiveness by learning multiple representations of token similarities.
  </div>
</body>
</html>
